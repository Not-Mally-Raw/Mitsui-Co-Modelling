{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "collapsed": true,
        "id": "SRHlCAlJWqAZ",
        "outputId": "5d2aa129-0357-4a80-aae8-1b6d829e4d87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please upload your kaggle.json file from Kaggle (download from your Kaggle account settings)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3258cf32-3ec3-41b2-842b-c1857c00bd9e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3258cf32-3ec3-41b2-842b-c1857c00bd9e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install kaggle --quiet\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import polars as pl\n",
        "\n",
        "# Step 1: Upload kaggle.json (API token from Kaggle)\n",
        "print(\"Please upload your kaggle.json file from Kaggle (download from your Kaggle account settings)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions list\n",
        "\n",
        "competition_name = \"mitsui-commodity-prediction-challenge\"\n",
        "\n",
        "!kaggle competitions download -c {competition_name}\n",
        "\n",
        "with zipfile.ZipFile(f\"{competition_name}.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"dataset\")\n",
        "\n",
        "print(\"\\nFiles in dataset directory:\")\n",
        "!ls dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "44IAU7a50Le-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data using pandas\n",
        "train_data_pd = pd.read_csv(\"dataset/train.csv\")\n",
        "train_labels_pd = pd.read_csv(\"dataset/train_labels.csv\")\n",
        "\n",
        "print(\"\\nTrain Data Info (Pandas):\")\n",
        "print(train_data_pd.info())\n",
        "print(\"\\nTrain Labels Info (Pandas):\")\n",
        "print(train_labels_pd.info())\n",
        "\n",
        "print(\"\\nTrain Data Sample (Pandas):\")\n",
        "print(train_data_pd.head())\n",
        "print(\"\\nTrain Labels Sample (Pandas):\")\n",
        "print(train_labels_pd.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bdffe717"
      },
      "outputs": [],
      "source": [
        "# Use pandas for summary statistics as the EDA was started with pandas\n",
        "print(\"\\nSummary Statistics for Train Data (Pandas):\")\n",
        "print(train_data_pd.describe())\n",
        "\n",
        "print(\"\\nSummary Statistics for Train Labels (Pandas):\")\n",
        "print(train_labels_pd.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6d8b8a4"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "\n",
        "# Load data using polars\n",
        "train_data_pl = pl.read_csv(\"dataset/train.csv\")\n",
        "train_labels_pl = pl.read_csv(\"dataset/train_labels.csv\")\n",
        "\n",
        "\n",
        "print(\"Train Data Schema (Polars):\")\n",
        "print(train_data_pl.schema)\n",
        "\n",
        "print(\"\\nTrain Data Sample (Polars):\")\n",
        "print(train_data_pl.head())\n",
        "\n",
        "print(\"\\nTrain Labels Schema (Polars):\")\n",
        "print(train_labels_pl.schema)\n",
        "\n",
        "print(\"\\nTrain Labels Sample (Polars):\")\n",
        "print(train_labels_pl.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6unODwHYG4T"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import polars as pl # Use polars\n",
        "\n",
        "# Calculate missing values using polars\n",
        "missing_train_data_pl = train_data_pl.null_count().unpivot().filter(pl.col(\"value\") > 0)\n",
        "missing_train_data_pl.columns = ['column', 'missing_count']\n",
        "\n",
        "\n",
        "missing_train_labels_pl = train_labels_pl.null_count().unpivot().filter(pl.col(\"value\") > 0)\n",
        "missing_train_labels_pl.columns = ['column', 'missing_count']\n",
        "\n",
        "# Visualize missing values\n",
        "fig_train_data = px.bar(missing_train_data_pl.to_pandas(), x=\"column\", y=\"missing_count\", title=\"Missing Values per Column in Train Data\")\n",
        "fig_train_data.update_layout(xaxis={'categoryorder':'total descending'}) # Order bars by missing count\n",
        "fig_train_data.show()\n",
        "\n",
        "fig_train_labels = px.bar(missing_train_labels_pl.to_pandas(), x=\"column\", y=\"missing_count\", title=\"Missing Values per Column in Train Labels\")\n",
        "fig_train_labels.update_layout(xaxis={'categoryorder':'total descending'}) # Order bars by missing count\n",
        "fig_train_labels.show()\n",
        "\n",
        "print(\"\\nMissing value counts in train_data_pl:\")\n",
        "print(missing_train_data_pl)\n",
        "print(\"\\nMissing value counts in train_labels_pl:\")\n",
        "print(missing_train_labels_pl)\n",
        "\n",
        "\n",
        "print(\"\\nStrategy for handling missing values:\")\n",
        "print(\"Based on the visualizations, we can see that some columns have a significant number of missing values.\")\n",
        "print(\"For time series data, forward fill (ffill) is a common imputation method to carry forward the last valid observation.\")\n",
        "print(\"We will apply ffill to handle missing values in both features and targets.\")\n",
        "print(\"Columns with an extremely high percentage of missing values might still be considered for dropping after imputation if ffill results in too many NaNs at the beginning.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf513aa3"
      },
      "outputs": [],
      "source": [
        "# Load data using polars\n",
        "train_data_pl = pl.read_csv(\"dataset/train.csv\")\n",
        "train_labels_pl = pl.read_csv(\"dataset/train_labels.csv\")\n",
        "\n",
        "print(\"\\nSummary Statistics for Train Data (Polars):\")\n",
        "print(train_data_pl.describe())\n",
        "\n",
        "print(\"\\nSummary Statistics for Train Labels (Polars):\")\n",
        "print(train_labels_pl.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4XrNM6FYRzS"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "selected_features_dist = [\n",
        "    \"LME_AH_Close\",\n",
        "    \"FX_EURUSD\",\n",
        "    \"US_Stock_SPYV_adj_close\",\n",
        "    \"JPX_Gold_Standard_Futures_Close\"\n",
        "]\n",
        "\n",
        "# Select a few target variables from train_labels_pl\n",
        "selected_targets_dist = [\"target_0\", \"target_1\", \"target_2\", \"target_3\"]\n",
        "\n",
        "# Filter selected features and targets to only include those present in the dataframes\n",
        "available_features_dist = [col for col in selected_features_dist if col in train_data_pl.columns]\n",
        "available_targets_dist = [col for col in selected_targets_dist if col in train_labels_pl.columns]\n",
        "\n",
        "print(f\"\\nSelected and available features for distribution analysis: {available_features_dist}\")\n",
        "print(f\"Selected and available targets for distribution analysis: {available_targets_dist}\")\n",
        "\n",
        "if not available_features_dist and not available_targets_dist:\n",
        "    print(\"No available features or targets to visualize distributions.\")\n",
        "else:\n",
        "    # Create and display histograms for selected features\n",
        "    for col in available_features_dist:\n",
        "        fig = px.histogram(\n",
        "            train_data_pl.to_pandas(), # Convert to pandas for plotly express\n",
        "            x=col,\n",
        "            title=f\"Distribution of {col} (Histogram)\",\n",
        "        )\n",
        "        fig.show()\n",
        "\n",
        "    # Create and display box plots for selected features\n",
        "    for col in available_features_dist:\n",
        "        fig = px.box(\n",
        "            train_data_pl.to_pandas(), # Convert to pandas for plotly express\n",
        "            y=col,\n",
        "            title=f\"Distribution of {col} (Box Plot)\",\n",
        "        )\n",
        "        fig.show()\n",
        "\n",
        "    # Create and display histograms for selected target variables\n",
        "    for col in available_targets_dist:\n",
        "        fig = px.histogram(\n",
        "            train_labels_pl.to_pandas(), # Convert to pandas for plotly express\n",
        "            x=col,\n",
        "            title=f\"Distribution of {col} (Histogram)\",\n",
        "        )\n",
        "        fig.show()\n",
        "\n",
        "    # Create and display box plots for selected target variables\n",
        "    for col in available_targets_dist:\n",
        "        fig = px.box(\n",
        "            train_labels_pl.to_pandas(), # Convert to pandas for plotly express\n",
        "            y=col,\n",
        "            title=f\"Distribution of {col} (Box Plot)\",\n",
        "        )\n",
        "        fig.show()\n",
        "\n",
        "    print(\"\\nAnalysis of the distributions:\")\n",
        "    print(\n",
        "        \"The histograms show the frequency distribution of the selected features and targets.\"\n",
        "    )\n",
        "    print(\n",
        "        \"The box plots provide a summary of the distribution, including median, quartiles, and potential outliers.\"\n",
        "    )\n",
        "    print(\n",
        "        \"Observations on skewness, multimodality, and outliers can be made by examining these plots.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a23a3f3"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import polars as pl # Use polars\n",
        "import pandas as pd # Keep pandas for correlation matrix calculation\n",
        "\n",
        "# Load data using polars (ensure it's loaded in this cell)\n",
        "train_data_pl = pl.read_csv(\"dataset/train.csv\")\n",
        "train_labels_pl = pl.read_csv(\"dataset/train_labels.csv\")\n",
        "\n",
        "# Select a subset of relevant numerical features from train_data_pl for correlation analysis\n",
        "# Filtering to top 10 low-missing-value numerical features (excluding 'date_id')\n",
        "missing_counts_pl = train_data_pl.null_count().unpivot()\n",
        "low_missing_features = missing_counts_pl.filter(pl.col('value') < 100).sort('value')['variable'].head(10).to_list()\n",
        "\n",
        "numerical_features_corr = [col for col, dtype in train_data_pl.schema.items() if col in low_missing_features and col != 'date_id' and dtype in [pl.Float64, pl.Int64]]\n",
        "\n",
        "# Select target variables (target_0 to target_3)\n",
        "target_columns_corr = [\"target_0\", \"target_1\", \"target_2\", \"target_3\"]\n",
        "\n",
        "# Filter selected features and targets to only include those present in the dataframes\n",
        "available_features_corr = [col for col in numerical_features_corr if col in train_data_pl.columns]\n",
        "available_targets_corr = [col for col in target_columns_corr if col in train_labels_pl.columns]\n",
        "\n",
        "\n",
        "print(f\"\\nSelected and available features for correlation analysis: {available_features_corr}\")\n",
        "print(f\"Selected and available targets for correlation analysis: {available_targets_corr}\")\n",
        "\n",
        "if not available_features_corr or not available_targets_corr:\n",
        "    print(\"No available features or targets to visualize correlations.\")\n",
        "else:\n",
        "    # Join the selected features and targets DataFrames on the 'date_id' column\n",
        "    # Drop rows where targets are all NaN to avoid issues in correlation calculation\n",
        "    # Convert to pandas for plotting\n",
        "    joined_df_pl = train_data_pl.select(['date_id'] + available_features_corr).join(\n",
        "        train_labels_pl.select(['date_id'] + available_targets_corr),\n",
        "        on='date_id',\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    # Drop rows with any NaN values after joining for cleaner correlation calculation\n",
        "    joined_df_pd = joined_df_pl.drop_nulls().to_pandas()\n",
        "\n",
        "\n",
        "    # Drop the 'date_id' column before calculating correlation\n",
        "    joined_df_pd = joined_df_pd.drop('date_id', axis=1)\n",
        "\n",
        "    # Calculate the Pearson correlation matrix\n",
        "    correlation_matrix_pd = joined_df_pd.corr(method='pearson')\n",
        "\n",
        "    # Create a heatmap of the correlation matrix using plotly.express.imshow.\n",
        "    # Enhance the heatmap with larger size and labeled axes\n",
        "    fig = px.imshow(\n",
        "        correlation_matrix_pd,\n",
        "        text_auto=False, # Set to True if you want to display correlation values on the heatmap\n",
        "        aspect=\"auto\",\n",
        "        color_continuous_scale=\"Viridis\",\n",
        "        title=\"Correlation Matrix of Selected Features and Target Variables\",\n",
        "        width=800, # Increase width\n",
        "        height=800 # Increase height\n",
        "    )\n",
        "\n",
        "    # Customize the heatmap with appropriate labels, title, and color scale for better readability.\n",
        "    fig.update_layout(\n",
        "        xaxis_title=\"Variables\",\n",
        "        yaxis_title=\"Variables\",\n",
        "        xaxis=dict(tickmode='array', tickvals=list(range(len(correlation_matrix_pd.columns))), ticktext=correlation_matrix_pd.columns, tickangle=-45),\n",
        "        yaxis=dict(tickmode='array', tickvals=list(range(len(correlation_matrix_pd.index))), ticktext=correlation_matrix_pd.index)\n",
        "    )\n",
        "\n",
        "\n",
        "    # Display the heatmap.\n",
        "    fig.show()\n",
        "\n",
        "    # Briefly analyze and summarize key observations from the correlation heatmap.\n",
        "    print(\"\\nAnalysis of the correlation heatmap:\")\n",
        "    print(\"- The heatmap shows the pairwise Pearson correlation coefficients between the selected features and target variables.\")\n",
        "    print(\"- Values close to 1 indicate a strong positive linear correlation, values close to -1 indicate a strong negative linear correlation, and values close to 0 indicate a weak or no linear correlation.\")\n",
        "    print(\"- Observe the strength and direction of correlations between different feature pairs and between features and target variables.\")\n",
        "    print(\"- Note any patterns or clusters of highly correlated variables.\")\n",
        "    print(\"- Pay close attention to correlations between features and target variables, as these can inform feature selection and model building.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e60dd6fc"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import polars as pl\n",
        "\n",
        "# 1. Select a subset of relevant numerical features from train_data_pl for time series plotting\n",
        "# Excluding 'date_id' and columns with very high missing values (based on previous EDA)\n",
        "# Let's select columns that had less than 100 missing values from the previous EDA\n",
        "missing_counts_pl = train_data_pl.null_count().unpivot()\n",
        "low_missing_features = missing_counts_pl.filter(pl.col('value') < 100)['variable'].to_list()\n",
        "\n",
        "\n",
        "# Ensure 'date_id' is not included and only select numerical columns\n",
        "numerical_features_ts = [col for col, dtype in train_data_pl.schema.items() if col in low_missing_features and col != 'date_id' and dtype in [pl.Float64, pl.Int64]]\n",
        "\n",
        "print(f\"\\nSelected and available features for time series plotting: {numerical_features_ts[:10]}...\") # Print a subset\n",
        "\n",
        "if not numerical_features_ts:\n",
        "    print(\"No available numerical features to visualize time series.\")\n",
        "else:\n",
        "    # Select a smaller subset for time series plotting to keep output manageable\n",
        "    features_to_plot_ts = numerical_features_ts[:5] # Plot first 5 numerical features\n",
        "\n",
        "    # Create and display time series plots for selected features\n",
        "    for col in features_to_plot_ts:\n",
        "        fig = px.line(\n",
        "            train_data_pl.to_pandas(), # Convert to pandas for plotly express\n",
        "            x=\"date_id\",\n",
        "            y=col,\n",
        "            title=f\"Time Series of {col}\",\n",
        "        )\n",
        "        fig.show()\n",
        "\n",
        "    print(\"\\nAnalysis of the time series plots:\")\n",
        "    print(\"- The line plots show the values of selected features over time (date_id).\")\n",
        "    print(\"- Observe trends, seasonality, and volatility in the time series data.\")\n",
        "    print(\"- Note any sudden drops or spikes that might indicate outliers or significant events.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04fdcb87"
      },
      "source": [
        "## Feature engineering (optional)\n",
        "\n",
        "### Subtask:\n",
        "Based on the EDA, consider creating new features that might be relevant for the prediction task (e.g., lagged features, rolling statistics).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "438973a8"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement feature engineering by creating lagged features and rolling statistics for selected features from `train_data_pl`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1169ffe"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "\n",
        "# Select features for engineering - focusing on numerical features with relatively low missing values\n",
        "# Using the numerical_features_corr list created in the correlation analysis step.\n",
        "# Ensure selected_features_for_fe is defined\n",
        "missing_counts_pl = train_data_pl.null_count().unpivot()\n",
        "low_missing_features = missing_counts_pl.filter(pl.col('value') < 100)['variable'].to_list()\n",
        "selected_features_for_fe = [col for col, dtype in train_data_pl.schema.items() if col in low_missing_features and col != 'date_id' and dtype in [pl.Float64, pl.Int64]]\n",
        "\n",
        "\n",
        "# Define window sizes for rolling statistics and lag\n",
        "rolling_window_5 = 5\n",
        "rolling_window_20 = 20 # Add another window size\n",
        "lag_days_1 = 1\n",
        "lag_days_5 = 5 # Add another lag period\n",
        "\n",
        "# Create new features using polars\n",
        "engineered_train_data_pl = train_data_pl.select(pl.all()) # Start with a copy of the original train_data_pl\n",
        "\n",
        "for col in selected_features_for_fe:\n",
        "    # Create lagged features\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).shift(lag_days_1).alias(f'{col}_lag_{lag_days_1}'),\n",
        "        pl.col(col).shift(lag_days_5).alias(f'{col}_lag_{lag_days_5}') # Add longer lag\n",
        "    ])\n",
        "\n",
        "    # Create rolling mean\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).rolling_mean(window_size=rolling_window_5).alias(f'{col}_rolling_mean_{rolling_window_5}'),\n",
        "        pl.col(col).rolling_mean(window_size=rolling_window_20).alias(f'{col}_rolling_mean_{rolling_window_20}') # Add longer rolling window\n",
        "    ])\n",
        "\n",
        "    # Create rolling standard deviation\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).rolling_std(window_size=rolling_window_5).alias(f'{col}_rolling_std_{rolling_window_5}'),\n",
        "        pl.col(col).rolling_std(window_size=rolling_window_20).alias(f'{col}_rolling_std_{rolling_window_20}') # Add longer rolling window\n",
        "    ])\n",
        "\n",
        "    # Add daily percentage change\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).pct_change().alias(f'{col}_pct_change')\n",
        "    ])\n",
        "\n",
        "# Display the schema and head of the DataFrame with new features\n",
        "print(\"\\nEngineered Train Data Info (Polars):\")\n",
        "print(engineered_train_data_pl.head()) # Polars head shows schema info\n",
        "print(f\"\\nNumber of columns after feature engineering: {len(engineered_train_data_pl.columns)}\")\n",
        "\n",
        "\n",
        "print(\"\\nFeature Engineering Steps and Rationale:\")\n",
        "print(f\"- Created lagged features (lag={lag_days_1}, {lag_days_5}) for selected numerical columns to capture temporal dependencies.\")\n",
        "print(f\"- Calculated rolling mean (window={rolling_window_5}, {rolling_window_20}) for selected numerical columns to smooth out noise and identify trends.\")\n",
        "print(f\"- Calculated rolling standard deviation (window={rolling_window_5}, {rolling_window_20}) for selected numerical columns to capture volatility.\")\n",
        "print(\"- Added daily percentage change to capture relative price movements.\")\n",
        "print(\"- Selected features for engineering are based on the previous EDA, focusing on columns with relatively low missing values and numerical types.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59a4237c"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import polars as pl # Use polars\n",
        "import pandas as pd # Keep pandas for StandardScaler\n",
        "\n",
        "\n",
        "print(\"Preprocessing Steps Based on EDA and Engineered Features:\")\n",
        "\n",
        "# 1. Handling Missing Values:\n",
        "print(\"\\n1. Handling Missing Values:\")\n",
        "print(\"- Applying forward fill (`fill_null(strategy='forward')`) to impute missing values in the engineered training data.\")\n",
        "print(\"- Applying backward fill (`fill_null(strategy='backward')`) as a fallback for initial NaNs.\")\n",
        "print(\"- This is a common approach for time series data to carry forward and backward the last/next valid observation.\")\n",
        "\n",
        "# Apply ffill and then bfill to the engineered features\n",
        "engineered_train_data_filled = engineered_train_data_pl.fill_null(strategy='forward').fill_null(strategy='backward')\n",
        "\n",
        "# Also apply ffill and then bfill to the target variables\n",
        "train_labels_filled = train_labels_pl.fill_null(strategy='forward').fill_null(strategy='backward')\n",
        "\n",
        "print(\"\\nMissing value counts after ffill and bfill in engineered_train_data_filled:\")\n",
        "missing_after_fill_features = engineered_train_data_filled.null_count().unpivot().filter(pl.col(\"value\") > 0)\n",
        "print(missing_after_fill_features)\n",
        "print(\"\\nMissing value counts after ffill and bfill in train_labels_filled:\")\n",
        "missing_after_fill_labels = train_labels_filled.null_count().unpivot().filter(pl.col(\"value\") > 0)\n",
        "print(missing_after_fill_labels)\n",
        "\n",
        "\n",
        "print(\"\\nNote: Columns with only missing values will still show as having missing values after fill. These columns might need to be dropped if they exist.\")\n",
        "\n",
        "\n",
        "# 2. Scaling Numerical Features:\n",
        "print(\"\\n2. Scaling Numerical Features:\")\n",
        "print(\"- Applying StandardScaler to standardize numerical features (mean=0, variance=1).\")\n",
        "print(\"- Scaling is important for many models, especially those sensitive to feature scales.\")\n",
        "print(\"- The scaler will be fitted on the training data and then used to transform both training and (later) testing data.\")\n",
        "\n",
        "# Identify numerical columns for scaling (exclude date_id and any remaining columns with NaNs if any)\n",
        "numerical_cols_for_scaling = [col for col, dtype in engineered_train_data_filled.schema.items() if dtype in [pl.Float64, pl.Int64] and col != 'date_id']\n",
        "\n",
        "# Check for any remaining NaNs before scaling and remove those columns if necessary\n",
        "# Polars .null_count() is efficient for this check\n",
        "cols_with_remaining_na = engineered_train_data_filled.select(numerical_cols_for_scaling).null_count().unpivot().filter(pl.col(\"value\") > 0)['variable'].to_list()\n",
        "\n",
        "if cols_with_remaining_na:\n",
        "    print(f\"Warning: Columns with remaining NaNs after fill will be excluded from scaling: {cols_with_remaining_na}\")\n",
        "    numerical_cols_for_scaling = [col for col in numerical_cols_for_scaling if col not in cols_with_remaining_na]\n",
        "\n",
        "# Convert to pandas for StandardScaler (as it's from scikit-learn)\n",
        "engineered_train_data_to_scale_pd = engineered_train_data_filled.select(numerical_cols_for_scaling).to_pandas()\n",
        "non_scaled_cols_pl = engineered_train_data_filled.select([col for col in engineered_train_data_filled.columns if col not in numerical_cols_for_scaling])\n",
        "\n",
        "\n",
        "if numerical_cols_for_scaling:\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit and transform the selected numerical columns\n",
        "    engineered_train_data_scaled_array = scaler.fit_transform(engineered_train_data_to_scale_pd)\n",
        "\n",
        "    # Convert scaled array back to Polars DataFrame\n",
        "    engineered_train_data_scaled_pl = pl.DataFrame(engineered_train_data_scaled_array, schema=numerical_cols_for_scaling)\n",
        "\n",
        "    # Combine scaled numerical features with non-scaled columns (like date_id if included)\n",
        "    engineered_train_data_scaled = non_scaled_cols_pl.hstack(engineered_train_data_scaled_pl)\n",
        "\n",
        "\n",
        "    print(\"\\nEngineered and Scaled Train Data Sample (Polars):\")\n",
        "    print(engineered_train_data_scaled.head())\n",
        "\n",
        "    print(\"\\nScaling applied to the following columns:\")\n",
        "    print(numerical_cols_for_scaling)\n",
        "else:\n",
        "    print(\"\\nNo numerical columns available for scaling after handling missing values.\")\n",
        "\n",
        "\n",
        "# 3. Handling Categorical Features:\n",
        "print(\"\\n3. Handling Categorical Features:\")\n",
        "print(\"- No explicit categorical features identified in this dataset (excluding 'date_id' which is an identifier).\")\n",
        "print(\"- No categorical encoding is required for this dataset.\")\n",
        "\n",
        "print(\"\\nRationale for Preprocessing:\")\n",
        "print(\"- **Missing Value Handling:** Imputation is necessary for models. ffill/bfill preserves temporal order.\")\n",
        "print(\"- **Scaling:** Standardizing features helps models sensitive to scale and can improve convergence.\")\n",
        "print(\"- **Categorical Encoding:** Not needed as there are no categorical features.\")\n",
        "\n",
        "print(\"\\nPrepared DataFrames for Modeling:\")\n",
        "print(\"- `engineered_train_data_scaled`: Contains engineered and scaled features with missing values imputed.\")\n",
        "print(\"- `train_labels_filled`: Contains target variables with missing values imputed.\")\n",
        "print(\"\\nThese dataframes are now ready for model selection and training, incorporating time-series cross-validation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94f95798"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Both `train_data_pl` (estimated size around 37.5 MB) and `train_labels_pl` (estimated size around 0.18 MB) were successfully loaded into Polars DataFrames.\n",
        "*   The `train_data_pl` schema contains an `Int64` column for `date_id` and numerous `Float64` columns representing financial indicators, while `train_labels_pl` contains `date_id` (`Int64`) and three `Float64` target columns (`target_0`, `target_1`, `target_2`, `target_3`).\n",
        "*   Both DataFrames contain missing values, with some columns having a significant number of missing entries as visualized by the bar charts.\n",
        "*   Summary statistics were successfully calculated and displayed for numerical columns in both DataFrames, showing varying ranges, means, and standard deviations.\n",
        "*   Histograms and box plots were successfully generated using Plotly for selected features (e.g., LME close prices, FX rates) and target variables (`target_0` to `target_3`), illustrating their distributions, including potential skewness and outliers.\n",
        "*   A correlation heatmap was generated showing the pairwise Pearson correlation coefficients between a subset of features with low missing values and the target variables, revealing the strength and direction of linear relationships.\n",
        "*   New features were successfully engineered by adding lagged features (lag=1) and rolling statistics (mean and standard deviation with a window of 5) to selected numerical columns in `train_data_pl`, creating `engineered_train_data_pl`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The next steps should focus on implementing the outlined preprocessing strategy, including handling missing values using time-series appropriate methods (like forward/backward fill) and potentially scaling numerical features, especially if scale-sensitive models are to be used.\n",
        "*   Proceed with model selection and training, utilizing the `engineered_train_data_pl` and the preprocessed `train_labels_pl` to predict the target variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "372188da"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* Both `train_data_pd` (estimated size around 8.3 MB) and `train_labels_pd` (estimated size around 6.4 MB) were successfully loaded into Pandas DataFrames.\n",
        "* The `train_data_pd` schema contains an `int64` column for `date_id` and numerous `float64` columns representing financial indicators, while `train_labels_pd` contains `date_id` (`int64`) and `float64` target columns (`target_0` to `target_423`).\n",
        "* Both DataFrames contain missing values, with some columns having a significant number of missing entries as visualized by the bar charts. Ordering the bars by missing count helps identify columns with the most missing data.\n",
        "* Summary statistics were successfully calculated and displayed for numerical columns in both DataFrames, showing varying ranges, means, and standard deviations.\n",
        "* Histograms and box plots were successfully generated using Plotly for selected features (e.g., LME close prices, FX rates, US stocks, JPX futures) and target variables (`target_0` to `target_3`), illustrating their distributions, including potential skewness and outliers.\n",
        "* A correlation heatmap was generated using a subset of features with relatively low missing values and the target variables, revealing the strength and direction of linear relationships. This helps in understanding potential predictors for the target variables.\n",
        "* New features were successfully engineered by adding lagged features (lag=1, 5), rolling statistics (mean and standard deviation with windows of 5 and 20), and daily percentage change to selected numerical columns in `train_data_pd`, creating `engineered_train_data_pd`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* The preprocessing steps for model training have been implemented within the existing cells:\n",
        "    * Missing values in `engineered_train_data_pd` and `train_labels_pd` were handled using forward fill (`fillna(method='ffill')`) to preserve the temporal order.\n",
        "    * Numerical features in `engineered_train_data_filled` were standardized using `StandardScaler` to prepare them for scale-sensitive models.\n",
        "* The dataframes `engineered_train_data_scaled` (features) and `train_labels_filled` (targets) are now prepared for model training.\n",
        "* The next steps should focus on model selection and training, incorporating time-series cross-validation using `TimeSeriesSplit` to ensure proper evaluation on future data. This will involve:\n",
        "    * Splitting the data into training and validation sets using `TimeSeriesSplit`.\n",
        "    * Selecting appropriate time-series forecasting models (e.g., XGBoost, LightGBM, or other models suitable for multi-output regression).\n",
        "    * Training the selected models on the training data.\n",
        "    * Evaluating model performance on the validation data using relevant metrics (e.g., Mean Squared Error, Mean Absolute Error).\n",
        "    * Potentially tuning hyperparameters to improve model performance.\n",
        "    * Finally, preparing predictions on the test data and generating a submission file in the specified format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e235d2af"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* Both `train_data_pl` (estimated size around 37.5 MB) and `train_labels_pl` (estimated size around 0.18 MB) were successfully loaded into Polars DataFrames.\n",
        "* The `train_data_pl` schema contains an `Int64` column for `date_id` and numerous `Float64` columns representing financial indicators, while `train_labels_pl` contains `date_id` (`Int64`) and `Float64` target columns (`target_0` to `target_423`).\n",
        "* Both DataFrames contain missing values, with some columns having a significant number of missing entries as visualized by the bar charts. Ordering the bars by missing count helps identify columns with the most missing data.\n",
        "* Summary statistics were successfully calculated and displayed for numerical columns in both DataFrames, showing varying ranges, means, and standard deviations.\n",
        "* Histograms and box plots were successfully generated using Plotly for selected features (e.g., LME close prices, FX rates, US stocks, JPX futures) and target variables (`target_0` to `target_3`), illustrating their distributions, including potential skewness and outliers.\n",
        "* Time series plots for a subset of features were generated, showing trends and patterns over time.\n",
        "* A correlation heatmap was generated using a subset of features with relatively low missing values and the target variables, revealing the strength and direction of linear relationships. This helps in understanding potential predictors for the target variables.\n",
        "* New features were successfully engineered by adding lagged features (lag=1, 5), rolling statistics (mean and standard deviation with windows of 5 and 20), and daily percentage change to selected numerical columns in `train_data_pl`, creating `engineered_train_data_pl`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* The preprocessing steps for model training have been implemented within the existing cells:\n",
        "  * Missing values in `engineered_train_data_pl` and `train_labels_pl` were handled using forward fill (`fill_null(strategy='forward')`) followed by backward fill (`fill_null(strategy='backward')`) to preserve the temporal order.\n",
        "  * Numerical features in `engineered_train_data_filled` were standardized using `StandardScaler` to prepare them for scale-sensitive models.\n",
        "* The dataframes `engineered_train_data_scaled` (features) and `train_labels_filled` (targets) are now prepared for model training.\n",
        "* The next steps should focus on model selection and training, incorporating time-series cross-validation using `TimeSeriesSplit` to ensure proper evaluation on future data. This will involve:\n",
        "  * Splitting the data into training and validation sets using `TimeSeriesSplit`.\n",
        "  * Selecting appropriate time-series forecasting models (e.g., XGBoost, LightGBM, or other models suitable for multi-output regression).\n",
        "  * Training the selected models on the training data.\n",
        "  * Evaluating model performance on the validation data using relevant metrics (e.g., Mean Squared Error, Mean Absolute Error).\n",
        "  * Potentially tuning hyperparameters to improve model performance.\n",
        "  * Finally, preparing predictions on the test data and generating a submission file in the specified format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc1ae87f"
      },
      "source": [
        "# Task\n",
        "Refine the provided Colab notebook JSON by editing existing cells in-place and adding 2-3 new code cells focused on advanced modeling. Build on the current structure: Enhance the VAR + XGBoost baseline with (1) an LSTM neural network for multi-output regression (using PyTorch or Keras, handling sequences with windowing), (2) hyperparameter tuning via GridSearchCV or Optuna with TimeSeriesSplit, and (3) ensemble blending (e.g., average predictions from VAR, XGBoost, LSTM). Include evaluation metrics (RMSE per target), feature importance plots, and a submission generator for test data (load test.csv, apply FE/preproc, predict, save as submission.csv). Ensure models account for multi-target outputs (target_0 to target_3 as commodity returns). Output the full updated notebook JSON."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c73fbb0"
      },
      "source": [
        "## Refine preprocessing and feature engineering (edit existing cells)\n",
        "\n",
        "### Subtask:\n",
        "Review and consolidate the existing preprocessing and feature engineering steps to ensure they are streamlined and efficient, focusing on the Polars implementation. Remove any redundant code or explanations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d9b7858"
      },
      "source": [
        "**Reasoning**:\n",
        "Consolidate Polars data loading and initial EDA into a single cell, removing redundant pandas code to streamline the notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "458804a9"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "\n",
        "# Load data using polars (ensure it's loaded in this cell)\n",
        "train_data_pl = pl.read_csv(\"dataset/train.csv\")\n",
        "train_labels_pl = pl.read_csv(\"dataset/train_labels.csv\")\n",
        "\n",
        "\n",
        "# Select features for engineering - focusing on numerical features with relatively low missing values\n",
        "# Using the missing_train_data_pl DataFrame created in the previous step to identify low missing features.\n",
        "missing_counts_pl = train_data_pl.null_count().unpivot()\n",
        "low_missing_features = missing_counts_pl.filter(pl.col('value') < 100)['variable'].to_list()\n",
        "\n",
        "# Ensure 'date_id' is not included and only select numerical columns from the original train_data_pl\n",
        "selected_features_for_fe = [col for col, dtype in train_data_pl.schema.items() if col in low_missing_features and col != 'date_id' and dtype in [pl.Float64, pl.Int64]]\n",
        "\n",
        "\n",
        "# Define window sizes for rolling statistics and lag\n",
        "rolling_window_5 = 5\n",
        "rolling_window_20 = 20 # Add another window size\n",
        "lag_days_1 = 1\n",
        "lag_days_5 = 5 # Add another lag period\n",
        "\n",
        "# Create new features using polars\n",
        "engineered_train_data_pl = train_data_pl.select(pl.all()) # Start with a copy of the original train_data_pl\n",
        "\n",
        "for col in selected_features_for_fe:\n",
        "    # Create lagged features\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).shift(lag_days_1).alias(f'{col}_lag_{lag_days_1}'),\n",
        "        pl.col(col).shift(lag_days_5).alias(f'{col}_lag_{lag_days_5}') # Add longer lag\n",
        "    ])\n",
        "\n",
        "    # Create rolling mean\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).rolling_mean(window_size=rolling_window_5).alias(f'{col}_rolling_mean_{rolling_window_5}'),\n",
        "        pl.col(col).rolling_mean(window_size=rolling_window_20).alias(f'{col}_rolling_mean_{rolling_window_20}') # Add longer rolling window\n",
        "    ])\n",
        "\n",
        "    # Create rolling standard deviation\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).rolling_std(window_size=rolling_window_5).alias(f'{col}_rolling_std_{rolling_window_5}'),\n",
        "        pl.col(col).rolling_std(window_size=rolling_window_20).alias(f'{col}_rolling_std_{rolling_window_20}') # Add longer rolling window\n",
        "    ])\n",
        "\n",
        "    # Add daily percentage change\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).pct_change().alias(f'{col}_pct_change')\n",
        "    ])\n",
        "\n",
        "# Display the schema and head of the DataFrame with new features\n",
        "print(\"\\nEngineered Train Data Info (Polars):\")\n",
        "print(engineered_train_data_pl.head()) # Polars head shows schema info\n",
        "print(f\"\\nNumber of columns after feature engineering: {len(engineered_train_data_pl.columns)}\")\n",
        "\n",
        "\n",
        "print(\"\\nFeature Engineering Steps and Rationale:\")\n",
        "print(f\"- Created lagged features (lag={lag_days_1}, {lag_days_5}) for selected numerical columns to capture temporal dependencies.\")\n",
        "print(f\"- Calculated rolling mean (window={rolling_window_5}, {rolling_window_20}) for selected numerical columns to smooth out noise and identify trends.\")\n",
        "print(f\"- Calculated rolling standard deviation (window={rolling_window_5}, {rolling_window_20}) for selected numerical columns to capture volatility.\")\n",
        "print(\"- Added daily percentage change to capture relative price movements.\")\n",
        "print(\"- Selected features for engineering are based on the previous EDA, focusing on columns with relatively low missing values and numerical types.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cf12793"
      },
      "source": [
        "**Reasoning**:\n",
        "Refine the preprocessing cell to ensure missing value handling and scaling are applied correctly to the engineered features and target variables using Polars and StandardScaler, and display the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b521e493"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import polars as pl # Use polars\n",
        "import pandas as pd # Keep pandas for StandardScaler\n",
        "\n",
        "# Load data using polars (ensure it's loaded in this cell)\n",
        "train_data_pl = pl.read_csv(\"dataset/train.csv\")\n",
        "train_labels_pl = pl.read_csv(\"dataset/train_labels.csv\")\n",
        "\n",
        "\n",
        "print(\"Preprocessing Steps Based on EDA and Engineered Features:\")\n",
        "\n",
        "# 1. Handling Missing Values:\n",
        "print(\"\\n1. Handling Missing Values:\")\n",
        "print(\"- Applying forward fill (`fill_null(strategy='forward')`) to impute missing values in the engineered training data.\")\n",
        "print(\"- Applying backward fill (`fill_null(strategy='backward')`) as a fallback for initial NaNs.\")\n",
        "print(\"- This is a common approach for time series data to carry forward and backward the last/next valid observation.\")\n",
        "\n",
        "# Apply ffill and then bfill to the engineered features\n",
        "engineered_train_data_filled = engineered_train_data_pl.fill_null(strategy='forward').fill_null(strategy='backward')\n",
        "\n",
        "# Also apply ffill and then bfill to the target variables\n",
        "train_labels_filled = train_labels_pl.fill_null(strategy='forward').fill_null(strategy='backward')\n",
        "\n",
        "print(\"\\nMissing value counts after ffill and bfill in engineered_train_data_filled:\")\n",
        "missing_after_fill_features = engineered_train_data_filled.null_count().unpivot().filter(pl.col(\"value\") > 0)\n",
        "print(missing_after_fill_features)\n",
        "print(\"\\nMissing value counts after ffill and bfill in train_labels_filled:\")\n",
        "missing_after_fill_labels = train_labels_filled.null_count().unpivot().filter(pl.col(\"value\") > 0)\n",
        "print(missing_after_fill_labels)\n",
        "\n",
        "\n",
        "print(\"\\nNote: Columns with only missing values will still show as having missing values after fill. These columns might need to be dropped if they exist.\")\n",
        "\n",
        "\n",
        "# 2. Scaling Numerical Features:\n",
        "print(\"\\n2. Scaling Numerical Features:\")\n",
        "print(\"- Applying StandardScaler to standardize numerical features (mean=0, variance=1).\")\n",
        "print(\"- Scaling is important for many models, especially those sensitive to feature scales.\")\n",
        "print(\"- The scaler will be fitted on the training data and then used to transform both training and (later) testing data.\")\n",
        "\n",
        "# Identify numerical columns for scaling (exclude date_id and any remaining columns with NaNs if any)\n",
        "numerical_cols_for_scaling = [col for col, dtype in engineered_train_data_filled.schema.items() if dtype in [pl.Float64, pl.Int64] and col != 'date_id']\n",
        "\n",
        "# Check for any remaining NaNs before scaling and remove those columns if necessary\n",
        "# Polars .null_count() is efficient for this check\n",
        "cols_with_remaining_na = engineered_train_data_filled.select(numerical_cols_for_scaling).null_count().unpivot().filter(pl.col(\"value\") > 0)['variable'].to_list()\n",
        "\n",
        "if cols_with_remaining_na:\n",
        "    print(f\"Warning: Columns with remaining NaNs after fill will be excluded from scaling: {cols_with_remaining_na}\")\n",
        "    numerical_cols_for_scaling = [col for col in numerical_cols_for_scaling if col not in cols_with_remaining_na]\n",
        "\n",
        "# Convert to pandas for StandardScaler (as it's from scikit-learn)\n",
        "engineered_train_data_to_scale_pd = engineered_train_data_filled.select(numerical_cols_for_scaling).to_pandas()\n",
        "non_scaled_cols_pl = engineered_train_data_filled.select([col for col in engineered_train_data_filled.columns if col not in numerical_cols_for_scaling])\n",
        "\n",
        "\n",
        "if numerical_cols_for_scaling:\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit and transform the selected numerical columns\n",
        "    engineered_train_data_scaled_array = scaler.fit_transform(engineered_train_data_to_scale_pd)\n",
        "\n",
        "    # Convert scaled array back to Polars DataFrame\n",
        "    engineered_train_data_scaled_pl = pl.DataFrame(engineered_train_data_scaled_array, schema=numerical_cols_for_scaling)\n",
        "\n",
        "    # Combine scaled numerical features with non-scaled columns (like date_id if included)\n",
        "    engineered_train_data_scaled = non_scaled_cols_pl.hstack(engineered_train_data_scaled_pl)\n",
        "\n",
        "\n",
        "    print(\"\\nEngineered and Scaled Train Data Sample (Polars):\")\n",
        "    print(engineered_train_data_scaled.head())\n",
        "\n",
        "    print(\"\\nScaling applied to the following columns:\")\n",
        "    print(numerical_cols_for_scaling)\n",
        "else:\n",
        "    print(\"\\nNo numerical columns available for scaling after handling missing values.\")\n",
        "\n",
        "\n",
        "# 3. Handling Categorical Features:\n",
        "print(\"\\n3. Handling Categorical Features:\")\n",
        "print(\"- No explicit categorical features identified in this dataset (excluding 'date_id' which is an identifier).\")\n",
        "print(\"- No categorical encoding is required for this dataset.\")\n",
        "\n",
        "print(\"\\nRationale for Preprocessing:\")\n",
        "print(\"- **Missing Value Handling:** Imputation is necessary for models. ffill/bfill preserves temporal order.\")\n",
        "print(\"- **Scaling:** Standardizing features helps models sensitive to scale and can improve convergence.\")\n",
        "print(\"- **Categorical Encoding:** Not needed as there are no categorical features.\")\n",
        "\n",
        "print(\"\\nPrepared DataFrames for Modeling:\")\n",
        "print(\"- `engineered_train_data_scaled`: Contains engineered and scaled features with missing values imputed.\")\n",
        "print(\"- `train_labels_filled`: Contains target variables with missing values imputed.\")\n",
        "print(\"\\nThese dataframes are now ready for model selection and training, incorporating time-series cross-validation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cae83688"
      },
      "source": [
        "## Implement time series cross-validation (new cell)\n",
        "\n",
        "### Subtask:\n",
        "Add a new cell to define and set up `TimeSeriesSplit` from scikit-learn for robust model evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3109c35f"
      },
      "source": [
        "**Reasoning**:\n",
        "Define and set up TimeSeriesSplit for robust model evaluation as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64840f8d"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Define the number of splits for time series cross-validation\n",
        "n_splits = 5\n",
        "\n",
        "# Instantiate TimeSeriesSplit\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "# Print the instantiated TimeSeriesSplit object\n",
        "print(f\"TimeSeriesSplit configured with {n_splits} splits:\")\n",
        "print(tscv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05cd14a0"
      },
      "source": [
        "## Implement xgboost model with hyperparameter tuning (new cell)\n",
        "\n",
        "### Subtask:\n",
        "Implement the XGBoost model for multi-output regression, including hyperparameter tuning using `GridSearchCV` with `TimeSeriesSplit`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79958555"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the XGBoost model for multi-output regression, including hyperparameter tuning using GridSearchCV with TimeSeriesSplit.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "28755dd4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "from joblib import parallel_backend  # For parallel processing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# Increased sample size for potentially better results\n",
        "train_data_pl = pl.read_csv(\"dataset/train.csv\").sample(n=int(pl.read_csv(\"dataset/train.csv\").shape[0] * 0.2), seed=42)  # 20% sample\n",
        "train_labels_pl = pl.read_csv(\"dataset/train_labels.csv\").sample(n=int(pl.read_csv(\"dataset/train_labels.csv\").shape[0] * 0.2), seed=42)\n",
        "\n",
        "# Parallel feature engineering with lazy evaluation\n",
        "missing_counts_pl = train_data_pl.null_count().unpivot()\n",
        "low_missing_features = missing_counts_pl.filter(pl.col('value') < 100)['variable'].to_list()\n",
        "selected_features_for_fe = [col for col, dtype in train_data_pl.schema.items() if col in low_missing_features and col != 'date_id' and dtype in [pl.Float64, pl.Int64]]\n",
        "\n",
        "# Apply feature engineering transformations directly using select\n",
        "engineered_train_data_pl = train_data_pl.lazy().select([\n",
        "    pl.col(col).alias(col) for col in train_data_pl.columns # Keep original columns\n",
        "] + [\n",
        "    pl.col(col).shift(1).alias(f'{col}_lag_1') for col in selected_features_for_fe\n",
        "] + [\n",
        "    pl.col(col).shift(5).alias(f'{col}_lag_5') for col in selected_features_for_fe\n",
        "] + [\n",
        "    pl.col(col).rolling_mean(window_size=5).alias(f'{col}_rolling_mean_5') for col in selected_features_for_fe\n",
        "] + [\n",
        "    pl.col(col).rolling_mean(window_size=20).alias(f'{col}_rolling_mean_20') for col in selected_features_for_fe\n",
        "] + [\n",
        "    pl.col(col).rolling_std(window_size=5).alias(f'{col}_rolling_std_5') for col in selected_features_for_fe\n",
        "] + [\n",
        "    pl.col(col).rolling_std(window_size=20).alias(f'{col}_rolling_std_20') for col in selected_features_for_fe\n",
        "] + [\n",
        "    pl.col(col).pct_change().alias(f'{col}_pct_change') for col in selected_features_for_fe\n",
        "]).collect()\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "engineered_train_data_filled = engineered_train_data_pl.fill_null(strategy='forward').fill_null(strategy='backward')\n",
        "train_labels_filled = train_labels_pl.fill_null(strategy='forward').fill_null(strategy='backward')\n",
        "\n",
        "# Check for and remove columns with remaining NaNs or infinities\n",
        "def check_for_invalid_values(df):\n",
        "    invalid_cols = []\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in [pl.Float64, pl.Float32]:\n",
        "            if df[col].is_nan().any() or df[col].is_infinite().any():\n",
        "                invalid_cols.append(col)\n",
        "    return invalid_cols\n",
        "\n",
        "invalid_feature_cols = check_for_invalid_values(engineered_train_data_filled.drop('date_id'))\n",
        "if invalid_feature_cols:\n",
        "    print(f\"Dropping feature columns with invalid values: {invalid_feature_cols}\")\n",
        "    engineered_train_data_cleaned = engineered_train_data_filled.drop(invalid_feature_cols)\n",
        "else:\n",
        "    engineered_train_data_cleaned = engineered_train_data_filled.clone()\n",
        "\n",
        "invalid_label_cols = check_for_invalid_values(train_labels_filled.drop('date_id'))\n",
        "if invalid_label_cols:\n",
        "    print(f\"Dropping target columns with invalid values: {invalid_label_cols}\")\n",
        "    train_labels_cleaned = train_labels_filled.drop(invalid_label_cols)\n",
        "else:\n",
        "    train_labels_cleaned = train_labels_filled.clone()\n",
        "\n",
        "\n",
        "numerical_cols_for_scaling = [col for col, dtype in engineered_train_data_cleaned.schema.items() if dtype in [pl.Float64, pl.Int64] and col != 'date_id']\n",
        "\n",
        "engineered_train_data_to_scale_pd = engineered_train_data_cleaned.select(numerical_cols_for_scaling).to_pandas()\n",
        "non_scaled_cols_pl = engineered_train_data_cleaned.select('date_id')\n",
        "\n",
        "\n",
        "if numerical_cols_for_scaling:\n",
        "    scaler = StandardScaler()\n",
        "    engineered_train_data_scaled_array = scaler.fit_transform(engineered_train_data_to_scale_pd)\n",
        "    engineered_train_data_scaled_pl = pl.DataFrame(engineered_train_data_scaled_array, schema=numerical_cols_for_scaling)\n",
        "    engineered_train_data_scaled = non_scaled_cols_pl.hstack(engineered_train_data_scaled_pl)\n",
        "else:\n",
        "    engineered_train_data_scaled = engineered_train_data_cleaned.clone() # Or handle as appropriate if no cols scaled\n",
        "\n",
        "# Define X and y\n",
        "X = engineered_train_data_scaled.drop('date_id').to_pandas()\n",
        "\n",
        "# Select a subset of target variables\n",
        "target_columns = [col for col in train_labels_cleaned.columns if col.startswith('target_')]\n",
        "selected_target_columns = target_columns[:4] # Select the first 4 target columns\n",
        "y = train_labels_cleaned.select(selected_target_columns).to_pandas()\n",
        "\n",
        "# Ensure columns in X and y are aligned in case of any processing discrepancies\n",
        "# This is a safeguard, assuming previous steps maintained order\n",
        "X = X.reindex(sorted(X.columns), axis=1)\n",
        "y = y.reindex(sorted(y.columns), axis=1)\n",
        "\n",
        "print(f\"Number of features (X): {X.shape[1]}\")\n",
        "print(f\"Number of target variables (y): {y.shape[1]}\")\n",
        "\n",
        "# Setup GridSearchCV with parallel processing\n",
        "tscv = TimeSeriesSplit(n_splits=3)  # Reduced folds for speed\n",
        "xgb = XGBRegressor(objective='reg:squarederror', random_state=42, n_estimators=100)\n",
        "param_grid = {\n",
        "    'estimator__learning_rate': [0.1],  # Simplified grid\n",
        "    'estimator__max_depth': [3]\n",
        "}\n",
        "multioutput_xgb = MultiOutputRegressor(xgb)\n",
        "\n",
        "with parallel_backend('loky', n_jobs=-1):  # Multi-processing backend\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=multioutput_xgb,\n",
        "        param_grid=param_grid,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        cv=tscv,\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "    print(\"Starting GridSearchCV for XGBoost...\")\n",
        "    grid_search.fit(X, y)\n",
        "\n",
        "print(\"\\nBest hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best score (Negative MSE):\", grid_search.best_score_)\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "print(\"\\nBest XGBoost model:\", best_xgb_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bb3b384"
      },
      "source": [
        "## Implement and train LSTM model (new cell)\n",
        "\n",
        "### Subtask:\n",
        "Implement and train an LSTM neural network for multi-output regression using PyTorch or Keras, handling sequences with windowing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "758cfe11"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement and train the LSTM neural network for multi-output regression using PyTorch, leveraging the sequenced data and the defined model architecture from the previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de3fc6e7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Define TimeSeriesDataset class\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.X[i], self.y[i]\n",
        "\n",
        "# Continue from the previous cell where the LSTM model and data tensors were defined\n",
        "\n",
        "# Define training parameters\n",
        "num_epochs = 50 # Can be tuned\n",
        "batch_size = 32 # Can be tuned\n",
        "\n",
        "# Create DataLoader for the sequenced data\n",
        "train_dataset = TimeSeriesDataset(X_tensors, y_tensors)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) # No shuffling for time series\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "lstm_model.to(device)\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Training loop\n",
        "print(\"\\nStarting LSTM model training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    lstm_model.train() # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # Move data to device\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = lstm_model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * batch_X.size(0) # Accumulate loss per sample\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
        "\n",
        "print(\"\\nLSTM model training finished.\")\n",
        "\n",
        "# Save the trained LSTM model\n",
        "# torch.save(lstm_model.state_dict(), 'lstm_model.pth')\n",
        "# print(\"\\nLSTM model saved as lstm_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a101660"
      },
      "source": [
        "## Define Target Columns and VAR Features (new cell)\n",
        "\n",
        "### Subtask:\n",
        "Define the list of target columns for prediction and the subset of features to be used for the VAR model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a66705ae"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the target columns and VAR features explicitly to ensure they are available for subsequent modeling and ensembling steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1ec97e5"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "\n",
        "# Load train labels to get all target column names\n",
        "train_labels_pl = pl.read_csv(\"dataset/train_labels.csv\")\n",
        "\n",
        "# Define the list of target columns to predict\n",
        "target_columns = [col for col in train_labels_pl.columns if col.startswith('target_')]\n",
        "selected_target_columns = target_columns[:4] # Select the first 4 target columns for now\n",
        "\n",
        "print(f\"Selected target columns: {selected_target_columns}\")\n",
        "\n",
        "# Define the subset of features to be used for the VAR model\n",
        "# These should be a subset of the features available in the engineered_train_data_scaled dataframe\n",
        "selected_features_for_var = ['LME_AH_Close', 'FX_EURUSD', 'US_Stock_SPYV_adj_close'] # Example features, can be tuned\n",
        "\n",
        "print(f\"Selected features for VAR model: {selected_features_for_var}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6f1cb5b"
      },
      "source": [
        "## Implement Ensemble Blending (new cell)\n",
        "\n",
        "### Subtask:\n",
        "Implement an ensemble blending strategy (e.g., simple averaging) using the predictions from the trained XGBoost, LSTM, and VAR models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6524d90e"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the ensemble blending strategy to combine the predictions from the individual models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a932d334"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import torch # Import torch for LSTM predictions\n",
        "import torch.nn as nn # Import nn for TimeSeriesDataset\n",
        "from torch.utils.data import Dataset, DataLoader # Import Dataset and DataLoader for TimeSeriesDataset\n",
        "from sklearn.preprocessing import StandardScaler # Import StandardScaler for test data preprocessing\n",
        "\n",
        "# Assume best_xgb_model, lstm_model, and best_var_model are trained and available from previous steps\n",
        "# Also assume scaler is available from the preprocessing step for scaling test data\n",
        "\n",
        "# Define TimeSeriesDataset class (copied from LSTM data preparation cell)\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.X[i], self.y[i]\n",
        "\n",
        "# Load the test data\n",
        "test_data_pl = pl.read_csv(\"dataset/test.csv\")\n",
        "\n",
        "# Apply the same feature engineering steps to the test data as applied to the training data\n",
        "# Ensure selected_features_for_fe is defined (can be copied from the FE cell)\n",
        "missing_counts_test_pl = test_data_pl.null_count().unpivot()\n",
        "low_missing_features_test = missing_counts_test_pl.filter(pl.col('value') < 100)['variable'].to_list()\n",
        "selected_features_for_fe_test = [col for col, dtype in test_data_pl.schema.items() if col in low_missing_features_test and col != 'date_id' and dtype in [pl.Float64, pl.Int64]]\n",
        "\n",
        "\n",
        "engineered_test_data_pl = test_data_pl.select(pl.all())\n",
        "for col in selected_features_for_fe_test:\n",
        "    engineered_test_data_pl = engineered_test_data_pl.with_columns([\n",
        "        pl.col(col).shift(1).alias(f'{col}_lag_1'),\n",
        "        pl.col(col).shift(5).alias(f'{col}_lag_5')\n",
        "    ])\n",
        "    engineered_test_data_pl = engineered_test_data_pl.with_columns([\n",
        "        pl.col(col).rolling_mean(window_size=5).alias(f'{col}_rolling_mean_5'),\n",
        "        pl.col(col).rolling_mean(window_size=20).alias(f'{col}_rolling_mean_20')\n",
        "    ])\n",
        "    engineered_test_data_pl = engineered_test_data_pl.with_columns([\n",
        "        pl.col(col).rolling_std(window_size=5).alias(f'{col}_rolling_std_5'),\n",
        "        pl.col(col).rolling_std(window_size=20).alias(f'{col}_rolling_std_20')\n",
        "    ])\n",
        "    engineered_test_data_pl = engineered_test_data_pl.with_columns([\n",
        "        pl.col(col).pct_change().alias(f'{col}_pct_change')\n",
        "    ])\n",
        "\n",
        "\n",
        "# Apply the same preprocessing steps (filling missing values and scaling) to the engineered test data\n",
        "engineered_test_data_filled = engineered_test_data_pl.fill_null(strategy='forward').fill_null(strategy='backward')\n",
        "\n",
        "# Identify numerical columns for scaling in the test data (exclude date_id and any remaining columns with NaNs if any)\n",
        "numerical_cols_for_scaling_test = [col for col, dtype in engineered_test_data_filled.schema.items() if dtype in [pl.Float64, pl.Int64] and col != 'date_id']\n",
        "\n",
        "# Check for any remaining NaNs or infinities before scaling and remove those columns if necessary\n",
        "cols_with_remaining_na_test = engineered_test_data_filled.select(numerical_cols_for_scaling_test).null_count().unpivot().filter(pl.col(\"value\") > 0)['variable'].to_list()\n",
        "\n",
        "if cols_with_remaining_na_test:\n",
        "    print(f\"Warning: Test feature columns with remaining invalid values: {cols_with_remaining_na_test}. These will be excluded from scaling and predictions.\")\n",
        "    numerical_cols_for_scaling_test = [col for col in numerical_cols_for_scaling_test if col not in cols_with_remaining_na_test]\n",
        "\n",
        "\n",
        "# Select the columns that were used for scaling in the training data and are present in the test data\n",
        "# Ensure the order of columns in the test data matches the order in the training data used for scaling\n",
        "scaled_train_cols = engineered_train_data_to_scale_pd.columns # Get columns used for scaling in train\n",
        "engineered_test_data_to_scale_pd = engineered_test_data_filled.select([col for col in scaled_train_cols if col in engineered_test_data_filled.columns]).to_pandas()\n",
        "engineered_test_data_to_scale_pd = engineered_test_data_to_scale_pd.reindex(columns=scaled_train_cols, fill_value=0) # Reindex to match train columns, fill missing with 0 or another strategy\n",
        "\n",
        "\n",
        "non_scaled_cols_test_pl = engineered_test_data_filled.select('date_id')\n",
        "\n",
        "\n",
        "if not engineered_test_data_to_scale_pd.empty and 'scaler' in locals(): # Check if dataframe is not empty and scaler object exists\n",
        "    # Transform the selected numerical columns using the *fitted* scaler from the training data\n",
        "    engineered_test_data_scaled_array = scaler.transform(engineered_test_data_to_scale_pd)\n",
        "\n",
        "    # Convert scaled array back to Polars DataFrame\n",
        "    engineered_test_data_scaled_pl = pl.DataFrame(engineered_test_data_scaled_array, schema=engineered_test_data_to_scale_pd.columns.tolist()) # Use the aligned columns\n",
        "\n",
        "    # Combine scaled numerical features with non-scaled columns (like date_id)\n",
        "    engineered_test_data_scaled = non_scaled_cols_test_pl.hstack(engineered_test_data_scaled_pl)\n",
        "\n",
        "else:\n",
        "    print(\"Warning: No numerical columns to scale in test data or scaler not found. Skipping scaling.\")\n",
        "    engineered_test_data_scaled = engineered_test_data_filled.clone() # Use filled data if scaling is skipped\n",
        "\n",
        "\n",
        "# Prepare test data for each model\n",
        "# XGBoost and VAR can use the engineered_test_data_scaled (excluding date_id)\n",
        "X_test_xgb_var = engineered_test_data_scaled.drop('date_id').to_pandas()\n",
        "\n",
        "# LSTM requires sequencing\n",
        "# Ensure the same sequence length and input features as the training LSTM data\n",
        "# Need to align test features with train features used for LSTM\n",
        "# Get the names of features used for LSTM training from the X_np dataframe\n",
        "lstm_feature_names = X_np.columns.tolist()\n",
        "\n",
        "# Select and order test features to match the training LSTM features\n",
        "X_test_lstm_np = engineered_test_data_scaled.select([col for col in lstm_feature_names if col in engineered_test_data_scaled.columns]).to_pandas()\n",
        "X_test_lstm_np = X_test_lstm_np.reindex(columns=lstm_feature_names, fill_value=0) # Reindex to match train columns, fill missing with 0\n",
        "\n",
        "\n",
        "# Create sequences for LSTM test data\n",
        "def create_sequences_test(features, seq_length):\n",
        "    X_seq = []\n",
        "    for i in range(len(features) - seq_length + 1): # +1 to include the last possible window\n",
        "        window = features.iloc[i:(i + seq_length)].values\n",
        "        X_seq.append(window)\n",
        "    return np.array(X_seq)\n",
        "\n",
        "# Ensure sequence_length and output_size are defined (from the LSTM model definition cell)\n",
        "# If not defined globally, you might need to get them from the trained lstm_model object or define them here\n",
        "# Assuming sequence_length and output_size are available from the previous LSTM cell\n",
        "\n",
        "if len(X_test_lstm_np) >= sequence_length:\n",
        "    X_test_lstm_sequences = create_sequences_test(X_test_lstm_np, sequence_length) # Use the same sequence_length as training\n",
        "\n",
        "    # Convert to PyTorch tensor\n",
        "    X_test_lstm_tensors = torch.tensor(X_test_lstm_sequences, dtype=torch.float32)\n",
        "\n",
        "    print(f\"\\nTest data features shape (XGBoost/VAR): {X_test_xgb_var.shape}\")\n",
        "    print(f\"Sequenced test data features shape (LSTM): {X_test_lstm_tensors.shape}\")\n",
        "\n",
        "\n",
        "    # Make predictions with each model\n",
        "    print(\"\\nMaking predictions with individual models...\")\n",
        "\n",
        "    # XGBoost Predictions\n",
        "    xgb_predictions = best_xgb_model.predict(X_test_xgb_var)\n",
        "    xgb_predictions_df = pd.DataFrame(xgb_predictions, columns=selected_target_columns) # Use selected target columns\n",
        "\n",
        "    # LSTM Predictions\n",
        "    # Ensure LSTM model is in evaluation mode\n",
        "    lstm_model.eval()\n",
        "    lstm_predictions = []\n",
        "    with torch.no_grad():\n",
        "        # Process test data in batches if necessary\n",
        "        test_dataset_lstm = TimeSeriesDataset(X_test_lstm_tensors, torch.zeros(X_test_lstm_tensors.shape[0], output_size)) # Dummy targets\n",
        "        test_loader_lstm = DataLoader(test_dataset_lstm, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        for batch_X, _ in test_loader_lstm:\n",
        "            batch_X = batch_X.to(device)\n",
        "            outputs = lstm_model(batch_X)\n",
        "            lstm_predictions.append(outputs.cpu().numpy())\n",
        "\n",
        "    lstm_predictions_array = np.concatenate(lstm_predictions, axis=0)\n",
        "\n",
        "    # Pad LSTM predictions to match the original test data length due to windowing\n",
        "    # The first (sequence_length - 1) predictions are not available\n",
        "    lstm_predictions_padded = np.pad(lstm_predictions_array, ((sequence_length - 1, 0), (0, 0)), mode='constant', constant_values=np.nan)\n",
        "    lstm_predictions_df = pd.DataFrame(lstm_predictions_padded, columns=selected_target_columns) # Use selected target columns\n",
        "\n",
        "\n",
        "    # VAR Predictions\n",
        "    # VAR model predicts the next 'lag_order' values. We need the last 'max_lag' observations from the training data\n",
        "    # to make the first prediction on the test data.\n",
        "    # For simplicity here, we will use a rolling forecast approach, predicting one step ahead\n",
        "    # starting from the end of the training data and extending into the test data.\n",
        "    # A more robust approach would involve retraining VAR on growing windows or using a forecast method that handles new data.\n",
        "\n",
        "    # Use the last 'max_lag' data points from the training data for the initial forecast\n",
        "    # Ensure var_data_pd from the VAR training cell is available\n",
        "    var_train_data_end = var_data_pd.tail(max_lag)\n",
        "\n",
        "    # Combine the end of training data with test data for rolling forecast\n",
        "    # Need to align columns and handle potential missing values in the test data for VAR\n",
        "    # Use the same features and targets selected for the VAR model during training\n",
        "    var_test_data_pl = engineered_test_data_scaled.select(['date_id'] + selected_features_for_var).join(\n",
        "        test_data_pl.select(['date_id'] + selected_target_columns), # Join with original test data for targets if needed for VAR structure\n",
        "        on='date_id',\n",
        "        how='left' # Use left join to keep all test dates\n",
        "    ).sort('date_id')\n",
        "\n",
        "    # Convert to pandas and ensure consistency with training data columns\n",
        "    var_test_data_pd = var_test_data_pl.drop('date_id').to_pandas()\n",
        "\n",
        "    # Check for and handle any remaining NaNs or inf in the VAR test data\n",
        "    if var_test_data_pd.isnull().sum().sum() > 0:\n",
        "        print(\"Warning: NaNs found in VAR test data. Using forward fill as a fallback.\")\n",
        "        var_test_data_pd = var_test_data_pd.fillna(method='ffill').fillna(method='bfill') # Fallback imputation\n",
        "\n",
        "    if np.isinf(var_test_data_pd).sum().sum() > 0:\n",
        "        print(\"Warning: Infinities found in VAR test data. Replacing with NaN and using forward fill.\")\n",
        "        var_test_data_pd = var_test_data_pd.replace([np.inf, -np.inf], np.nan).fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "\n",
        "    # Combine train end and test data for VAR prediction input\n",
        "    var_predict_input_pd = pd.concat([var_train_data_end, var_test_data_pd], ignore_index=True)\n",
        "\n",
        "    # Make rolling predictions with VAR model\n",
        "    var_predictions = []\n",
        "    # The VAR model was fitted on data with 'max_lag' removed from the start.\n",
        "    # So the effective start index for prediction using the combined data is 'max_lag'.\n",
        "    for i in range(len(var_test_data_pd)):\n",
        "        # The input for prediction is the last 'best_var_model.k_ar' observations\n",
        "        # from the combined train_end and test data.\n",
        "        input_data = var_predict_input_pd[i:i + best_var_model.k_ar] # Use k_ar attribute for lag order\n",
        "        forecast = best_var_model.forecast(y=input_data.values, steps=1)\n",
        "        var_predictions.append(forecast[0]) # forecast returns an array of arrays, take the first (and only) step\n",
        "\n",
        "    var_predictions_array = np.array(var_predictions)\n",
        "    var_predictions_df = pd.DataFrame(var_predictions_array, columns=selected_target_columns) # Use selected target columns\n",
        "\n",
        "\n",
        "    print(f\"\\nXGBoost predictions shape: {xgb_predictions_df.shape}\")\n",
        "    print(f\"LSTM predictions shape: {lstm_predictions_df.shape}\")\n",
        "    print(f\"VAR predictions shape: {var_predictions_df.shape}\")\n",
        "\n",
        "    # Ensemble Blending (Simple Averaging)\n",
        "    # Align the predictions based on date_id or index if necessary.\n",
        "    # Since we processed data sequentially, the indices should align.\n",
        "    # We need to handle the NaNs introduced by LSTM windowing and VAR initial lag.\n",
        "    # For simple averaging, we will only average non-NaN predictions.\n",
        "\n",
        "    # Create a list of prediction dataframes\n",
        "    predictions_list = [xgb_predictions_df, lstm_predictions_df, var_predictions_df]\n",
        "\n",
        "    # Combine predictions into a single DataFrame for averaging\n",
        "    # Use a common index (e.g., date_id from test_data_pl)\n",
        "    test_date_ids = test_data_pl['date_id'].to_pandas()\n",
        "\n",
        "    # Assign the common index to each prediction DataFrame\n",
        "    xgb_predictions_df.index = test_date_ids\n",
        "    lstm_predictions_df.index = test_date_ids\n",
        "    var_predictions_df.index = test_date_ids\n",
        "\n",
        "\n",
        "    # Stack the dataframes and group by index (date_id) and column (target) to calculate the mean\n",
        "    # Need to handle potential column mismatches if models predicted different targets\n",
        "    # Ensure all prediction dataframes have the same target columns\n",
        "    for pred_df in predictions_list:\n",
        "        if not pred_df.columns.equals(selected_target_columns):\n",
        "            print(f\"Warning: Prediction DataFrame has mismatched columns. Expected: {selected_target_columns}, Got: {pred_df.columns.tolist()}\")\n",
        "            # Align columns - pad with NaNs if a target is missing in a model's prediction\n",
        "            for target_col in selected_target_columns:\n",
        "                if target_col not in pred_df.columns:\n",
        "                    pred_df[target_col] = np.nan\n",
        "            pred_df = pred_df[selected_target_columns] # Reorder columns to match\n",
        "\n",
        "\n",
        "    # Concatenate the prediction dataframes\n",
        "    all_predictions_df = pd.concat(predictions_list)\n",
        "\n",
        "    # Calculate the mean prediction for each date_id and target variable\n",
        "    ensemble_predictions_df = all_predictions_df.groupby(all_predictions_df.index).mean()\n",
        "\n",
        "    print(\"\\nEnsemble predictions sample:\")\n",
        "    print(ensemble_predictions_df.head())\n",
        "\n",
        "    # The ensemble_predictions_df now contains the blended predictions for the selected target columns.\n",
        "    # We need to merge this back with the original test data structure to create the submission file.\n",
        "\n",
        "    # Load the sample submission file to get the required format\n",
        "    sample_submission_pl = pl.read_csv(\"dataset/kaggle_evaluation/sample_submission.csv\")\n",
        "    sample_submission_pd = sample_submission_pl.to_pandas()\n",
        "\n",
        "    # Merge the ensemble predictions with the sample submission based on date_id and target column\n",
        "    # The sample submission is in a long format (date_id, target, value)\n",
        "    # The ensemble_predictions_df is in a wide format (date_id as index, target as columns)\n",
        "\n",
        "    # Convert ensemble_predictions_df to long format\n",
        "    ensemble_predictions_long_df = ensemble_predictions_df.stack().reset_index()\n",
        "    ensemble_predictions_long_df.columns = ['date_id', 'target', 'value']\n",
        "\n",
        "\n",
        "    # Merge with the sample submission to get all required target columns (even if not predicted)\n",
        "    # Use a right merge to keep all rows from the sample submission\n",
        "    submission_df = pd.merge(\n",
        "        sample_submission_pd[['date_id', 'target']],\n",
        "        ensemble_predictions_long_df,\n",
        "        on=['date_id', 'target'],\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Fill any missing values in the 'value' column (for targets not in selected_target_columns) with a default (e.g., 0)\n",
        "    submission_df['value'] = submission_df['value'].fillna(0) # Fill with 0 for unpredicted targets\n",
        "\n",
        "    # Ensure the submission file has the correct columns and order\n",
        "    submission_df = submission_df[['date_id', 'target', 'value']]\n",
        "\n",
        "    # Save the submission file\n",
        "    submission_df.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "    print(\"\\nEnsemble predictions generated and saved to submission.csv\")\n",
        "    print(\"\\nSubmission file head:\")\n",
        "    print(submission_df.head())\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping ensemble prediction and submission generation due to insufficient test data for sequencing or missing scaler.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f319c6a5"
      },
      "source": [
        "## Implement and train VAR model (new cell)\n",
        "\n",
        "### Subtask:\n",
        "Implement and train a VAR (Vector Autoregression) model for multi-output regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "065b4669"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement and train the VAR model as part of the ensemble approach for multi-output regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab812604"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.api import VAR\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "\n",
        "# Load data using polars (ensure it's loaded in this cell)\n",
        "train_data_pl = pl.read_csv(\"dataset/train.csv\")\n",
        "train_labels_pl = pl.read_csv(\"dataset/train_labels.csv\")\n",
        "\n",
        "# Select features for engineering (ensure this block is present and executed)\n",
        "missing_counts_pl = train_data_pl.null_count().unpivot()\n",
        "low_missing_features = missing_counts_pl.filter(pl.col('value') < 100)['variable'].to_list()\n",
        "selected_features_for_fe = [col for col, dtype in train_data_pl.schema.items() if col in low_missing_features and col != 'date_id' and dtype in [pl.Float64, pl.Int64]]\n",
        "engineered_train_data_pl = train_data_pl.select(pl.all())\n",
        "for col in selected_features_for_fe:\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).shift(1).alias(f'{col}_lag_1'),\n",
        "        pl.col(col).shift(5).alias(f'{col}_lag_5')\n",
        "    ])\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).rolling_mean(window_size=5).alias(f'{col}_rolling_mean_5'),\n",
        "        pl.col(col).rolling_mean(window_size=20).alias(f'{col}_rolling_mean_20')\n",
        "    ])\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).rolling_std(window_size=5).alias(f'{col}_rolling_std_5'),\n",
        "        pl.col(col).rolling_std(window_size=20).alias(f'{col}_rolling_std_20')\n",
        "    ])\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).pct_change().alias(f'{col}_pct_change')\n",
        "    ])\n",
        "\n",
        "# Apply preprocessing (ensure this block is present and executed)\n",
        "engineered_train_data_filled = engineered_train_data_pl.fill_null(strategy='forward').fill_null(strategy='backward')\n",
        "train_labels_filled = train_labels_pl.fill_null(strategy='forward').fill_null(strategy='backward')\n",
        "\n",
        "# Check for and remove columns with remaining NaNs or infinities\n",
        "def check_for_invalid_values(df):\n",
        "    invalid_cols = []\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in [pl.Float64, pl.Float32]:\n",
        "            if df[col].is_nan().any() or df[col].is_infinite().any():\n",
        "                invalid_cols.append(col)\n",
        "    return invalid_cols\n",
        "\n",
        "invalid_feature_cols = check_for_invalid_values(engineered_train_data_filled.drop('date_id'))\n",
        "if invalid_feature_cols:\n",
        "    print(f\"Dropping feature columns with invalid values: {invalid_feature_cols}\")\n",
        "    engineered_train_data_cleaned = engineered_train_data_filled.drop(invalid_feature_cols)\n",
        "else:\n",
        "    engineered_train_data_cleaned = engineered_train_data_filled.clone()\n",
        "\n",
        "invalid_label_cols = check_for_invalid_values(train_labels_filled.drop('date_id'))\n",
        "if invalid_label_cols:\n",
        "    print(f\"Dropping target columns with invalid values: {invalid_label_cols}\")\n",
        "    train_labels_cleaned = train_labels_filled.drop(invalid_label_cols)\n",
        "else:\n",
        "    train_labels_cleaned = train_labels_filled.clone()\n",
        "\n",
        "\n",
        "numerical_cols_for_scaling = [col for col, dtype in engineered_train_data_cleaned.schema.items() if dtype in [pl.Float64, pl.Int64] and col != 'date_id']\n",
        "\n",
        "engineered_train_data_to_scale_pd = engineered_train_data_cleaned.select(numerical_cols_for_scaling).to_pandas()\n",
        "non_scaled_cols_pl = engineered_train_data_cleaned.select('date_id')\n",
        "\n",
        "if numerical_cols_for_scaling:\n",
        "    scaler = StandardScaler()\n",
        "    engineered_train_data_scaled_array = scaler.fit_transform(engineered_train_data_to_scale_pd)\n",
        "    engineered_train_data_scaled_pl = pl.DataFrame(engineered_train_data_scaled_array, schema=numerical_cols_for_scaling)\n",
        "    engineered_train_data_scaled = non_scaled_cols_pl.hstack(engineered_train_data_scaled_pl)\n",
        "else:\n",
        "    engineered_train_data_scaled = engineered_train_data_cleaned.clone() # Or handle as appropriate if no cols scaled\n",
        "\n",
        "\n",
        "# Select a subset of features and targets for VAR model\n",
        "# VAR model requires a stationary time series and can be sensitive to the number of variables\n",
        "# Let's select a smaller, potentially more stable subset of features and the selected target variables\n",
        "selected_features_for_var = ['LME_AH_Close', 'FX_EURUSD', 'US_Stock_SPYV_adj_close'] # Example features, can be tuned\n",
        "selected_target_columns = [col for col in train_labels_cleaned.columns if col.startswith('target_')][:4] # Use the same subset of targets as XGBoost\n",
        "\n",
        "# Combine selected features and targets for VAR model\n",
        "# Ensure data is sorted by date_id for time series modeling\n",
        "var_data_pl = engineered_train_data_scaled.select(['date_id'] + selected_features_for_var).join(\n",
        "    train_labels_cleaned.select(['date_id'] + selected_target_columns),\n",
        "    on='date_id',\n",
        "    how='inner' # Use inner join to ensure we have both features and targets\n",
        ").sort('date_id')\n",
        "\n",
        "# Convert to pandas DataFrame for statsmodels VAR\n",
        "var_data_pd = var_data_pl.drop('date_id').to_pandas()\n",
        "\n",
        "# Check for and handle any remaining NaNs or inf in the VAR data\n",
        "if var_data_pd.isnull().sum().sum() > 0:\n",
        "    print(\"Warning: NaNs found in VAR data after join and fill. Using forward fill as a fallback.\")\n",
        "    var_data_pd = var_data_pd.fillna(method='ffill').fillna(method='bfill') # Fallback imputation\n",
        "\n",
        "if np.isinf(var_data_pd).sum().sum() > 0:\n",
        "    print(\"Warning: Infinities found in VAR data. Replacing with NaN and using forward fill.\")\n",
        "    var_data_pd = var_data_pd.replace([np.inf, -np.inf], np.nan).fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "\n",
        "print(f\"\\nVAR model data shape: {var_data_pd.shape}\")\n",
        "print(\"\\nVAR model data sample:\")\n",
        "print(var_data_pd.head())\n",
        "\n",
        "# Fit the VAR model\n",
        "# Determine the optimal lag order (can use AIC, BIC, etc.)\n",
        "# For simplicity, let's start with a small, fixed lag order\n",
        "max_lag = 5 # Can be tuned\n",
        "\n",
        "print(f\"\\nFitting VAR model with max lag {max_lag}...\")\n",
        "model = VAR(var_data_pd)\n",
        "\n",
        "# Select lag order based on AIC\n",
        "try:\n",
        "    var_results = model.fit(maxlags=max_lag, ic='aic')\n",
        "    print(\"\\nVAR model fitting complete.\")\n",
        "    print(var_results.summary())\n",
        "\n",
        "    # Store the fitted VAR model\n",
        "    best_var_model = var_results\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error fitting VAR model: {e}\")\n",
        "    best_var_model = None # Set to None if fitting fails\n",
        "\n",
        "if best_var_model:\n",
        "    print(\"\\nVAR model trained successfully.\")\n",
        "else:\n",
        "    print(\"\\nVAR model training failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "012cda01"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.multioutput import MultiOutputRegressor # Import MultiOutputRegressor\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "\n",
        "# Load data using polars (ensure it's loaded in this cell)\n",
        "train_data_pl = pl.read_csv(\"dataset/train.csv\")\n",
        "train_labels_pl = pl.read_csv(\"dataset/train_labels.csv\")\n",
        "\n",
        "# Select features for engineering (ensure this block is present and executed)\n",
        "missing_counts_pl = train_data_pl.null_count().unpivot()\n",
        "low_missing_features = missing_counts_pl.filter(pl.col('value') < 100)['variable'].to_list()\n",
        "selected_features_for_fe = [col for col, dtype in train_data_pl.schema.items() if col in low_missing_features and col != 'date_id' and dtype in [pl.Float64, pl.Int64]]\n",
        "engineered_train_data_pl = train_data_pl.select(pl.all())\n",
        "for col in selected_features_for_fe:\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).shift(1).alias(f'{col}_lag_1'),\n",
        "        pl.col(col).shift(5).alias(f'{col}_lag_5')\n",
        "    ])\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).rolling_mean(window_size=5).alias(f'{col}_rolling_mean_5'),\n",
        "        pl.col(col).rolling_mean(window_size=20).alias(f'{col}_rolling_mean_20')\n",
        "    ])\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).rolling_std(window_size=5).alias(f'{col}_rolling_std_5'),\n",
        "        pl.col(col).rolling_std(window_size=20).alias(f'{col}_rolling_std_20')\n",
        "    ])\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).pct_change().alias(f'{col}_pct_change')\n",
        "    ])\n",
        "\n",
        "# Apply preprocessing (ensure this block is present and executed)\n",
        "engineered_train_data_filled = engineered_train_data_pl.fill_null(strategy='forward').fill_null(strategy='backward')\n",
        "train_labels_filled = train_labels_pl.fill_null(strategy='forward').fill_null(strategy='backward')\n",
        "\n",
        "numerical_cols_for_scaling = [col for col, dtype in engineered_train_data_filled.schema.items() if dtype in [pl.Float64, pl.Int64] and col != 'date_id']\n",
        "cols_with_remaining_na = engineered_train_data_filled.select(numerical_cols_for_scaling).null_count().unpivot().filter(pl.col(\"value\") > 0)['variable'].to_list()\n",
        "if cols_with_remaining_na:\n",
        "    numerical_cols_for_scaling = [col for col in numerical_cols_for_scaling if col not in cols_with_remaining_na]\n",
        "\n",
        "engineered_train_data_to_scale_pd = engineered_train_data_filled.select(numerical_cols_for_scaling).to_pandas()\n",
        "non_scaled_cols_pl = engineered_train_data_filled.select([col for col in engineered_train_data_filled.columns if col not in numerical_cols_for_scaling])\n",
        "\n",
        "if numerical_cols_for_scaling:\n",
        "    scaler = StandardScaler()\n",
        "    engineered_train_data_scaled_array = scaler.fit_transform(engineered_train_data_to_scale_pd)\n",
        "    engineered_train_data_scaled_pl = pl.DataFrame(engineered_train_data_scaled_array, schema=numerical_cols_for_scaling)\n",
        "    engineered_train_data_scaled = non_scaled_cols_pl.hstack(engineered_train_data_scaled_pl)\n",
        "else:\n",
        "    engineered_train_data_scaled = engineered_train_data_filled.clone() # Or handle as appropriate if no cols scaled\n",
        "\n",
        "\n",
        "# Define features (X) and targets (y)\n",
        "# Drop 'date_id' from features as it's not a feature for the model\n",
        "X = engineered_train_data_scaled.drop('date_id').to_pandas()\n",
        "y = train_labels_filled.drop('date_id').to_pandas()\n",
        "\n",
        "# Ensure columns in X and y are aligned in case of any processing discrepancies\n",
        "# This is a safeguard, assuming previous steps maintained order\n",
        "X = X.reindex(sorted(X.columns), axis=1)\n",
        "y = y.reindex(sorted(y.columns), axis=1)\n",
        "\n",
        "\n",
        "# Instantiate an XGBoost model.\n",
        "# For multi-output, XGBoostRegressor often works directly, but MultiOutputRegressor provides a robust wrapper.\n",
        "# We will use MultiOutputRegressor here for clarity and broader compatibility.\n",
        "# A smaller number of estimators for hyperparameter tuning to save time\n",
        "xgb = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Define a hyperparameter grid for tuning.\n",
        "# Keep the grid relatively small for the first pass due to computational cost.\n",
        "param_grid = {\n",
        "    'estimator__n_estimators': [100, 200],\n",
        "    'estimator__learning_rate': [0.05, 0.1],\n",
        "    'estimator__max_depth': [3, 5],\n",
        "}\n",
        "\n",
        "# Wrap XGBoostRegressor with MultiOutputRegressor\n",
        "multioutput_xgb = MultiOutputRegressor(xgb)\n",
        "\n",
        "\n",
        "# Set up GridSearchCV with TimeSeriesSplit\n",
        "# Use 'neg_mean_squared_error' as the scoring metric for GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=multioutput_xgb,\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=tscv, # Use the TimeSeriesSplit object\n",
        "    n_jobs=-1, # Use all available cores\n",
        "    verbose=2 # Print progress\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV to your features (X) and targets (y).\n",
        "print(\"Starting GridSearchCV for XGBoost...\")\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best hyperparameters found and the corresponding best score.\n",
        "print(\"\\nBest hyperparameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "print(\"\\nBest cross-validation score (Negative MSE):\")\n",
        "print(grid_search.best_score_)\n",
        "\n",
        "# Train the final XGBoost model using the best hyperparameters found.\n",
        "# The best estimator from GridSearchCV is already trained on the full training data\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "\n",
        "print(\"\\nBest XGBoost model (MultiOutputRegressor) trained using best hyperparameters:\")\n",
        "print(best_xgb_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64620938"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler # Import StandardScaler\n",
        "\n",
        "# Load data using polars (ensure it's loaded in this cell)\n",
        "train_data_pl = pl.read_csv(\"dataset/train.csv\")\n",
        "train_labels_pl = pl.read_csv(\"dataset/train_labels.csv\")\n",
        "\n",
        "# Select features for engineering (ensure this block is present and executed)\n",
        "missing_counts_pl = train_data_pl.null_count().unpivot()\n",
        "low_missing_features = missing_counts_pl.filter(pl.col('value') < 100)['variable'].to_list()\n",
        "selected_features_for_fe = [col for col, dtype in train_data_pl.schema.items() if col in low_missing_features and col != 'date_id' and dtype in [pl.Float64, pl.Int64]]\n",
        "engineered_train_data_pl = train_data_pl.select(pl.all())\n",
        "for col in selected_features_for_fe:\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).shift(1).alias(f'{col}_lag_1'),\n",
        "        pl.col(col).shift(5).alias(f'{col}_lag_5')\n",
        "    ])\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).rolling_mean(window_size=5).alias(f'{col}_rolling_mean_5'),\n",
        "        pl.col(col).rolling_mean(window_size=20).alias(f'{col}_rolling_mean_20')\n",
        "    ])\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).rolling_std(window_size=5).alias(f'{col}_rolling_std_5'),\n",
        "        pl.col(col).rolling_std(window_size=20).alias(f'{col}_rolling_std_20')\n",
        "    ])\n",
        "    engineered_train_data_pl = engineered_train_data_pl.with_columns([\n",
        "        pl.col(col).pct_change().alias(f'{col}_pct_change')\n",
        "    ])\n",
        "\n",
        "# Apply preprocessing (ensure this block is present and executed)\n",
        "engineered_train_data_filled = engineered_train_data_pl.fill_null(strategy='forward').fill_null(strategy='backward')\n",
        "train_labels_filled = train_labels_pl.fill_null(strategy='forward').fill_null(strategy='backward')\n",
        "\n",
        "numerical_cols_for_scaling = [col for col, dtype in engineered_train_data_filled.schema.items() if dtype in [pl.Float64, pl.Int64] and col != 'date_id']\n",
        "cols_with_remaining_na = engineered_train_data_filled.select(numerical_cols_for_scaling).null_count().unpivot().filter(pl.col(\"value\") > 0)['variable'].to_list()\n",
        "if cols_with_remaining_na:\n",
        "    numerical_cols_for_scaling = [col for col in numerical_cols_for_scaling if col not in cols_with_remaining_na]\n",
        "\n",
        "engineered_train_data_to_scale_pd = engineered_train_data_filled.select(numerical_cols_for_scaling).to_pandas()\n",
        "non_scaled_cols_pl = engineered_train_data_filled.select([col for col in engineered_train_data_filled.columns if col not in numerical_cols_for_scaling])\n",
        "\n",
        "if numerical_cols_for_scaling:\n",
        "    scaler = StandardScaler()\n",
        "    engineered_train_data_scaled_array = scaler.fit_transform(engineered_train_data_to_scale_pd)\n",
        "    engineered_train_data_scaled_pl = pl.DataFrame(engineered_train_data_scaled_array, schema=numerical_cols_for_scaling)\n",
        "    engineered_train_data_scaled = non_scaled_cols_pl.hstack(engineered_train_data_scaled_pl)\n",
        "else:\n",
        "    engineered_train_data_scaled = engineered_train_data_filled.clone() # Or handle as appropriate if no cols scaled\n",
        "\n",
        "\n",
        "# Define the sequence window size for the LSTM\n",
        "sequence_length = 10 # This can be tuned\n",
        "\n",
        "# Prepare data for LSTM (windowing)\n",
        "def create_sequences(features, targets, seq_length):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(features) - seq_length):\n",
        "        # Features are from i to i + seq_length\n",
        "        # Target is at i + seq_length (predicting the next step)\n",
        "        window = features.iloc[i:(i + seq_length)].values\n",
        "        label = targets.iloc[i + seq_length].values\n",
        "        X_seq.append(window)\n",
        "        y_seq.append(label)\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Convert engineered_train_data_scaled and train_labels_filled to Pandas for easier handling with numpy and torch\n",
        "X_np = engineered_train_data_scaled.drop('date_id').to_pandas()\n",
        "y_np = train_labels_filled.drop('date_id').to_pandas()\n",
        "\n",
        "# Create sequences\n",
        "X_sequences, y_sequences = create_sequences(X_np, y_np, sequence_length)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensors = torch.tensor(X_sequences, dtype=torch.float32)\n",
        "y_tensors = torch.tensor(y_sequences, dtype=torch.float32)\n",
        "\n",
        "print(f\"Original features shape: {X_np.shape}\")\n",
        "print(f\"Original targets shape: {y_np.shape}\")\n",
        "print(f\"Sequenced features shape: {X_tensors.shape}\")\n",
        "print(f\"Sequenced targets shape: {y_tensors.shape}\")\n",
        "\n",
        "\n",
        "# Define the LSTM model\n",
        "class LSTMRegressor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTMRegressor, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden and cell states\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :]) # Use the output from the last time step\n",
        "        return out\n",
        "\n",
        "# Define model parameters\n",
        "input_size = X_tensors.shape[-1] # Number of features per time step\n",
        "hidden_size = 64 # Can be tuned\n",
        "num_layers = 2 # Can be tuned\n",
        "output_size = y_tensors.shape[-1] # Number of target variables\n",
        "\n",
        "lstm_model = LSTMRegressor(input_size, hidden_size, num_layers, output_size)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001) # Learning rate can be tuned\n",
        "\n",
        "# Prepare data loaders (optional but good practice for training)\n",
        "# Using TimeSeriesSplit for splitting will be done during training loop\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.X[i], self.y[i]\n",
        "\n",
        "# Note: Data loading and training loop will be implemented in the next step\n",
        "print(\"\\nLSTM model defined and data prepared for sequence processing.\")\n",
        "print(\"\\nLSTM model instance created:\")\n",
        "print(lstm_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}